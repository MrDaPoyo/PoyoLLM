{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "cell_id": "a3d906151a3a4542a4c66a8d120991e1",
                "deepnote_cell_type": "markdown"
            },
            "source": "# The PoyoModel3000\nThis jupyter notebook will contain the model itself.",
            "block_group": "dab970e58fe747cf8aec1897066b6c09"
        },
        {
            "cell_type": "code",
            "metadata": {
                "source_hash": "c861570b",
                "execution_start": 1745505459667,
                "execution_millis": 1237,
                "execution_context_id": "5c07c2dd-95b5-4d75-8e71-757b6df90be6",
                "cell_id": "e412021656df4e7480fba04c246bd3da",
                "deepnote_cell_type": "code"
            },
            "source": "import sys\nsys.path.append('.')\nfrom minbpe import BasicTokenizer\n\ntokenizer = BasicTokenizer()\ntokenizer.load(model_file=\"./output/tokenizer/poyo_tokenizer.model\")\ndef get_vocab_size(tokenizer: BasicTokenizer) -> int:\n    vocab = tokenizer.vocab\n    special_tokens = tokenizer.special_tokens\n\n    return len(vocab) + len(special_tokens)",
            "block_group": "3ce9bffae873417d808bf281cd5e9256",
            "execution_count": 1,
            "outputs": [],
            "outputs_reference": null,
            "content_dependencies": {
                "codeHash": "c861570b",
                "usedVariables": [
                    "tokenizer",
                    "BasicTokenizer",
                    "sys",
                    "vocab",
                    "special_tokens"
                ],
                "importedModules": [
                    "BasicTokenizer",
                    "sys"
                ],
                "definedVariables": [
                    "special_tokens",
                    "get_vocab_size",
                    "vocab",
                    "tokenizer"
                ]
            }
        },
        {
            "cell_type": "code",
            "metadata": {
                "source_hash": "490f889b",
                "execution_start": 1745505460967,
                "execution_millis": 5910,
                "execution_context_id": "5c07c2dd-95b5-4d75-8e71-757b6df90be6",
                "cell_id": "c0ca098cf393494e85dadddaa2e70019",
                "deepnote_cell_type": "code"
            },
            "source": "import torch\ntorch.manual_seed(6969)\n\nblock_size = 1024\nn_embd = 384\nn_head = 12\nn_layer = 12\ndropout = 0.2\nvocab_size = get_vocab_size(tokenizer)\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\ndevice = torch.cuda.is_available() and torch.cuda.current_device() if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\na = torch.tensor([0], dtype=torch.float32, device=device)  # Fixed the syntax error in tensor creation",
            "block_group": "df65438f0432478895f1abf94e3848c8",
            "execution_count": 2,
            "outputs": [
                {
                    "name": "stdout",
                    "text": "CUDA available: False\nUsing device: cpu\n",
                    "output_type": "stream"
                }
            ],
            "outputs_reference": "dbtable:cell_outputs/7a46ed88-b318-4d47-aefe-33d4e68cdc5a",
            "content_dependencies": {
                "codeHash": "490f889b",
                "usedVariables": [
                    "torch",
                    "device",
                    "get_vocab_size",
                    "tokenizer"
                ],
                "importedModules": [
                    "torch"
                ],
                "definedVariables": [
                    "block_size",
                    "dropout",
                    "a",
                    "device",
                    "n_embd",
                    "n_head",
                    "vocab_size",
                    "n_layer"
                ]
            }
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "cell_id": "94ce748d498b4272abffba6ca025d952",
                "deepnote_cell_type": "markdown"
            },
            "source": "# The Head (drama here)",
            "block_group": "18481d8d30be4adc8da5e41e0fcef52b"
        },
        {
            "cell_type": "code",
            "metadata": {
                "source_hash": "eb098e53",
                "execution_start": 1745505466935,
                "execution_millis": 0,
                "execution_context_id": "5c07c2dd-95b5-4d75-8e71-757b6df90be6",
                "cell_id": "88ad53a635554fc29f38f5f371fd08bf",
                "deepnote_cell_type": "code"
            },
            "source": "from typing import Optional, Tuple\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\n\nclass Head(nn.Module):\n    \"\"\" one head of self-attention btw I do not have an attention span \"\"\"\n\n    def __init__(self, head_size: int) -> None:\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(\n            torch.ones(block_size, block_size)))\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        # input of size (batch, time-step, channels)\n        # output of size (batch, time-step, head size)\n        _, T, _ = x.shape\n        k = self.key(x)   # (B,T,hs)\n        q = self.query(x)  # (B,T,hs)\n        # compute attention scores (\"affinities\")\n        # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n        weights = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5\n        weights = weights.masked_fill(\n            self.tril[:T, :T] == 0, float('-inf'))  # (B, T, T)\n        weights = F.softmax(weights, dim=-1)  # (B, T, T)\n        weights = self.dropout(weights)\n        # perform the weighted aggregation of the values\n        v = self.value(x)  # (B,T,hs)\n        out = weights @ v  # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n        return out",
            "block_group": "f6f9a736e0bd4c4db015c169520abc8c",
            "execution_count": 3,
            "outputs": [],
            "outputs_reference": null,
            "content_dependencies": {
                "error": {
                    "type": "AttributeError",
                    "message": "'Attribute' object has no attribute 'id'"
                },
                "codeHash": "eb098e53",
                "usedVariables": [],
                "importedModules": [],
                "definedVariables": []
            }
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "cell_id": "96a90593ffcf47098340aa59fd8f8244",
                "deepnote_cell_type": "markdown"
            },
            "source": "# Multi-Head Attention",
            "block_group": "4e0dcbb5f099475f88b0ec2731615dfd"
        },
        {
            "cell_type": "code",
            "metadata": {
                "source_hash": "7e703f76",
                "execution_start": 1745505467004,
                "execution_millis": 1,
                "execution_context_id": "5c07c2dd-95b5-4d75-8e71-757b6df90be6",
                "cell_id": "4e4c9caa494840fc949c6c8e9f989474",
                "deepnote_cell_type": "code"
            },
            "source": "class MultiHeadAttention(nn.Module):\n    \"\"\" multiple heads of self-attention in parallel \"\"\"\n\n    def __init__(self, num_heads: int, head_size: int) -> None:\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        self.projection = nn.Linear(head_size * num_heads, n_embd)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.dropout(self.projection(out))\n        return out",
            "block_group": "c62afd5ea56d4cc1a9d062c4723079b1",
            "execution_count": 4,
            "outputs": [],
            "outputs_reference": null,
            "content_dependencies": {
                "error": {
                    "type": "AttributeError",
                    "message": "'Attribute' object has no attribute 'id'"
                },
                "codeHash": "7e703f76",
                "usedVariables": [],
                "importedModules": [],
                "definedVariables": []
            }
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "cell_id": "6e967f140032491e85d320df2abe650d",
                "deepnote_cell_type": "markdown"
            },
            "source": "# The BLOCK ",
            "block_group": "962f18c755db4c2fbe3d0ed14b86a4e3"
        },
        {
            "cell_type": "code",
            "metadata": {
                "source_hash": "f37d4556",
                "execution_start": 1745505467078,
                "execution_millis": 1,
                "execution_context_id": "5c07c2dd-95b5-4d75-8e71-757b6df90be6",
                "cell_id": "8e735145a2004069b6ca6417420b7012",
                "deepnote_cell_type": "code"
            },
            "source": "class FeedFoward(nn.Module):\n    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n\n    def __init__(self, n_embd: int) -> None:\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),\n            nn.ReLU(),\n            nn.Linear(4 * n_embd, n_embd),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.net(x)\n\n\nclass Block(nn.Module):\n    \"\"\" Transformer block: communication followed by computation \"\"\"\n\n    def __init__(self, n_embd: int, n_head: int) -> None:\n        # n_embd: embedding dimension, n_head: the number of heads we'd like\n        super().__init__()\n        head_size = n_embd // n_head\n        self.self_attention = MultiHeadAttention(n_head, head_size)\n        self.feed_forward = FeedFoward(n_embd)\n        self.layer_norm_1 = nn.LayerNorm(n_embd)\n        self.layer_norm_2 = nn.LayerNorm(n_embd)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = x + self.self_attention(self.layer_norm_1(x))\n        x = x + self.feed_forward(self.layer_norm_2(x))\n        return x",
            "block_group": "87c48363c2d849b7843042bd9d3a1116",
            "execution_count": 5,
            "outputs": [],
            "outputs_reference": null,
            "content_dependencies": {
                "error": {
                    "type": "AttributeError",
                    "message": "'Attribute' object has no attribute 'id'"
                },
                "codeHash": "f37d4556",
                "usedVariables": [],
                "importedModules": [],
                "definedVariables": []
            }
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "cell_id": "58fc0627b02649aba44751ad5bdfc046",
                "deepnote_cell_type": "markdown"
            },
            "source": "# Assembling the model",
            "block_group": "e90f24af1808461f8d60540058a76ed2"
        },
        {
            "cell_type": "code",
            "metadata": {
                "source_hash": "73b62f32",
                "execution_start": 1745505467144,
                "execution_millis": 1,
                "execution_context_id": "5c07c2dd-95b5-4d75-8e71-757b6df90be6",
                "cell_id": "56cb2e400a7542ca8b45339a5676fcb2",
                "deepnote_cell_type": "code"
            },
            "source": "class GPTLanguageModel(nn.Module):\n\n    def __init__(self) -> None:\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.blocks = nn.Sequential(\n            *[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n        self.final_layer_norm = nn.LayerNorm(n_embd)\n        self.final_linear_layer = nn.Linear(n_embd, vocab_size)\n\n        self.apply(self._init_weights)\n\n    def _init_weights(self, module: nn.Module) -> None:\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\n    def forward(self, input_tokens: torch.Tensor, targets: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n        \"\"\"\n        Forward pass of the model.\n\n        Args:\n            input_tokens: Tensor of token indices of shape (batch_size, sequence_length)\n            targets: Optional tensor of target token indices of same shape as input_tokens\n\n        Returns:\n            Tuple of (logits, loss) where logits has shape (batch_size, sequence_length, vocab_size)\n            and loss is optional cross-entropy loss if targets are provided\n        \"\"\"\n\n        B, T = input_tokens.shape\n\n        # input_tokens and targets are both (B,T) tensor of integers\n        token_embedding = self.token_embedding_table(input_tokens)  # (B,T,C)\n        positional_embedding = self.position_embedding_table(\n            torch.arange(T, device=device))  # (T,C)\n        x = token_embedding + positional_embedding  # (B,T,C)\n        x = self.blocks(x)  # (B,T,C)\n        x = self.final_layer_norm(x)  # (B,T,C)\n        logits = self.final_linear_layer(x)  # (B,T,vocab_size)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, input_tokens: torch.Tensor, max_new_tokens: int) -> torch.Tensor:\n        \"\"\"\n                Generate new tokens given a context.\n\n                Args:>ns: Starting token indices of shape (batch_size, sequence_length)\n                        max_new_tokens: Number of new tokens to generate\n\n                Returns:\n                        Tensor of token indices of shape (batch_size, sequence_length + max_new_tokens)\n                \"\"\"\n\n        # input_tokens is (B, T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # crop input_tokens to the last block_size tokens\n            cropped_input = input_tokens[:, -block_size:]\n            # get the predictions\n            logits, _ = self(cropped_input)\n            # focus only on the last time step\n            logits = logits[:, -1, :]  # becomes (B, C)\n            # apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1)  # (B, C)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n            # append sampled index to the running sequence\n            input_tokens = torch.cat(\n                (input_tokens, idx_next), dim=1)  # (B, T+1)\n        return input_tokens",
            "block_group": "eb43e7cdfe28405fb1d2cde5981471d4",
            "execution_count": 6,
            "outputs": [],
            "outputs_reference": null,
            "content_dependencies": {
                "error": {
                    "type": "AttributeError",
                    "message": "'Attribute' object has no attribute 'id'"
                },
                "codeHash": "73b62f32",
                "usedVariables": [],
                "importedModules": [],
                "definedVariables": []
            }
        },
        {
            "cell_type": "code",
            "metadata": {
                "source_hash": "f310bba6",
                "execution_start": 1745505467215,
                "execution_millis": 3158,
                "execution_context_id": "5c07c2dd-95b5-4d75-8e71-757b6df90be6",
                "cell_id": "a20642231579486eb85909edc065f177",
                "deepnote_cell_type": "code"
            },
            "source": "model = GPTLanguageModel()\nmodel = model.to(device)\n# print the number of parameters in the model\nprint(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')",
            "block_group": "237cc96d2215441c9a384ba868ec3ed9",
            "execution_count": 7,
            "outputs": [
                {
                    "name": "stdout",
                    "text": "22.467336 M parameters\n",
                    "output_type": "stream"
                }
            ],
            "outputs_reference": "dbtable:cell_outputs/cecc22ef-d7f7-47d6-a71e-2d1b837b6fe4",
            "content_dependencies": {
                "codeHash": "f310bba6",
                "usedVariables": [
                    "model",
                    "p",
                    "GPTLanguageModel",
                    "device"
                ],
                "importedModules": [],
                "definedVariables": [
                    "model",
                    "p"
                ]
            }
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "cell_id": "202dad3095ed497db15b09ce4bf8d69a",
                "deepnote_cell_type": "text-cell-p"
            },
            "source": "I'll help you create a training loop for the PoyoLLM model. First, let's create a training function that includes learning rate scheduling, gradient clipping, and proper device handling.",
            "block_group": "703f30f39e26415a959bbe816ee6a3c2"
        },
        {
            "cell_type": "code",
            "metadata": {
                "source_hash": "bcbc406",
                "execution_start": 1745505470435,
                "execution_millis": 0,
                "sql_integration_id": "",
                "execution_context_id": "5c07c2dd-95b5-4d75-8e71-757b6df90be6",
                "deepnote_variable_name": "",
                "cell_id": "8aae586bd45e4117a876fd1816eaad64",
                "deepnote_cell_type": "code"
            },
            "source": "import torch\nfrom torch.optim import AdamW\nfrom torch.nn import functional as F\nimport numpy as np\nfrom tqdm import tqdm\n\ndef train_model(\n    model,\n    train_data,\n    val_data=None,\n    n_epochs=5,\n    batch_size=32,\n    learning_rate=3e-4,\n    max_grad_norm=1.0,\n    warmup_steps=2000,\n    eval_interval=500,\n    save_interval=1000,\n    checkpoint_dir='checkpoints'\n):\n    # Create optimizer\n    optimizer = AdamW(model.parameters(), lr=learning_rate)\n    \n    # Learning rate scheduler\n    def get_lr(step, warmup_steps, learning_rate):\n        # Linear warmup followed by cosine decay\n        if step < warmup_steps:\n            return learning_rate * step / warmup_steps\n        return learning_rate * 0.5 * (1 + np.cos(np.pi * (step - warmup_steps) / (n_epochs * len(train_data) - warmup_steps)))\n    \n    # Training loop\n    step = 0\n    best_val_loss = float('inf')\n    \n    for epoch in range(n_epochs):\n        model.train()\n        pbar = tqdm(range(0, len(train_data), batch_size), desc=f'Epoch {epoch+1}/{n_epochs}')\n        \n        for i in pbar:\n            # Get batch\n            batch_data = train_data[i:i+batch_size]\n            if isinstance(batch_data, torch.Tensor):\n                x = batch_data\n                y = batch_data\n            else:\n                x = torch.tensor(batch_data, dtype=torch.long, device=device)\n                y = torch.tensor(batch_data, dtype=torch.long, device=device)\n            \n            # Forward pass\n            logits, loss = model(x, y)\n            \n            # Backward pass\n            optimizer.zero_grad(set_to_none=True)\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n            \n            # Update learning rate\n            lr = get_lr(step, warmup_steps, learning_rate)\n            for param_group in optimizer.param_groups:\n                param_group['lr'] = lr\n            \n            # Update weights\n            optimizer.step()\n            \n            # Update progress bar\n            pbar.set_postfix({'loss': f'{loss.item():.4f}', 'lr': f'{lr:.2e}'})\n            \n            # Evaluate on validation set\n            if val_data is not None and step % eval_interval == 0:\n                model.eval()\n                val_losses = []\n                with torch.no_grad():\n                    for j in range(0, len(val_data), batch_size):\n                        val_batch = val_data[j:j+batch_size]\n                        if isinstance(val_batch, torch.Tensor):\n                            val_x = val_batch\n                            val_y = val_batch\n                        else:\n                            val_x = torch.tensor(val_batch, dtype=torch.long, device=device)\n                            val_y = torch.tensor(val_batch, dtype=torch.long, device=device)\n                        _, val_loss = model(val_x, val_y)\n                        val_losses.append(val_loss.item())\n                \n                avg_val_loss = np.mean(val_losses)\n                print(f'\\nStep {step}: Validation loss: {avg_val_loss:.4f}')\n                \n                # Save best model\n                if avg_val_loss < best_val_loss:\n                    best_val_loss = avg_val_loss\n                    torch.save({\n                        'epoch': epoch,\n                        'model_state_dict': model.state_dict(),\n                        'optimizer_state_dict': optimizer.state_dict(),\n                        'loss': best_val_loss,\n                    }, f'{checkpoint_dir}/best_model.pt')\n                \n                model.train()\n            \n            # Save periodic checkpoint\n            if step % save_interval == 0:\n                torch.save({\n                    'epoch': epoch,\n                    'model_state_dict': model.state_dict(),\n                    'optimizer_state_dict': optimizer.state_dict(),\n                    'loss': loss.item(),\n                }, f'{checkpoint_dir}/checkpoint_{step}.pt')\n            \n            step += 1\n            \n    return model\n\nprint(\"Training function created successfully!\")",
            "block_group": "aac046e6fcb745d38aead84b7bceca85",
            "execution_count": 8,
            "outputs": [
                {
                    "name": "stdout",
                    "text": "Training function created successfully!\n",
                    "output_type": "stream"
                }
            ],
            "outputs_reference": "dbtable:cell_outputs/600aea94-fee2-4923-b9ae-5e741c467cc9",
            "content_dependencies": null
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "cell_id": "6222c4720018434e80237cf3f060910e",
                "deepnote_cell_type": "text-cell-p"
            },
            "source": "I'll create a helper function that loads the vocabulary and prepares the training data. Let me build this in steps.",
            "block_group": "3811afe4ff25443ca3c3277de340310a"
        },
        {
            "cell_type": "code",
            "metadata": {
                "source_hash": "b9a3a993",
                "execution_start": 1745505470494,
                "execution_millis": 2,
                "sql_integration_id": "",
                "execution_context_id": "5c07c2dd-95b5-4d75-8e71-757b6df90be6",
                "deepnote_variable_name": "",
                "cell_id": "a3d25c8de01949d6b4f970b38781826d",
                "deepnote_cell_type": "code"
            },
            "source": "def load_and_split_data(tokenizer, split_ratio=0.9, sequence_length=block_size):\n    # Load and process text\n    vocab_list = []\n    for word, value in tokenizer.vocab.items():\n        if isinstance(word, str):  # Only process string tokens\n            vocab_list.append(word)\n    \n    # Create a long text by joining words with spaces\n    text = ' '.join(vocab_list)\n    \n    # Convert text to token indices using tokenizer\n    tokens = tokenizer.encode(text)  # Changed tokenize to encode\n    \n    # Convert to tensor\n    data = torch.tensor(tokens, dtype=torch.long, device=device)\n    \n    # Create sequences of fixed length\n    n_sequences = max(1, len(data) - sequence_length)  # Ensure at least one sequence\n    sequences = []\n    for i in range(n_sequences):\n        seq = data[i:i+sequence_length]\n        # Pad sequence if necessary\n        if len(seq) < sequence_length:\n            padding = torch.zeros(sequence_length - len(seq), dtype=torch.long, device=device)\n            seq = torch.cat([seq, padding])\n        sequences.append(seq)\n    \n    sequences = torch.stack(sequences)\n    \n    # Split into train and validation\n    split_idx = int(len(sequences) * split_ratio)\n    train_data = sequences[:split_idx]\n    val_data = sequences[split_idx:]\n    \n    print(f\"Total sequences: {len(sequences)}\")\n    print(f\"Training sequences: {len(train_data)}\")\n    print(f\"Validation sequences: {len(val_data)}\")\n    \n    return train_data, val_data\n\n# Test the function\ntrain_data, val_data = load_and_split_data(tokenizer)",
            "block_group": "9d55cd9299f44360bad1a828f9fab3e1",
            "execution_count": 9,
            "outputs": [
                {
                    "name": "stdout",
                    "text": "Total sequences: 1\nTraining sequences: 0\nValidation sequences: 1\n",
                    "output_type": "stream"
                }
            ],
            "outputs_reference": "dbtable:cell_outputs/406ac0d2-1ade-4a7a-a67d-5686e7b78127",
            "content_dependencies": null
        },
        {
            "cell_type": "code",
            "metadata": {
                "source_hash": "5304a81f",
                "execution_start": 1745505470553,
                "execution_millis": 5,
                "sql_integration_id": "",
                "execution_context_id": "5c07c2dd-95b5-4d75-8e71-757b6df90be6",
                "deepnote_variable_name": "",
                "cell_id": "9c01e9a14d8e4f9dbcb5ef1bad71f50b",
                "deepnote_cell_type": "code"
            },
            "source": "def load_and_split_data(tokenizer, split_ratio=0.9, sequence_length=block_size):\n    # Get all text from vocab (converting integers to strings if necessary)\n    vocab_list = []\n    for word in tokenizer.vocab.keys():\n        if isinstance(word, int):\n            vocab_list.append(str(word))\n        else:\n            vocab_list.append(word)\n    \n    # Join all words with spaces\n    text = \" \".join(vocab_list)\n    \n    # Convert text to token indices\n    tokens = tokenizer.encode(text)\n    \n    # Convert to tensor\n    data = torch.tensor(tokens, dtype=torch.long, device=device)\n    \n    # Create sequences of fixed length\n    n_sequences = len(data) - sequence_length\n    if n_sequences <= 0:\n        raise ValueError(f\"Data length ({len(data)}) is shorter than sequence length ({sequence_length})\")\n    \n    sequences = torch.stack([data[i:i+sequence_length] for i in range(n_sequences)])\n    \n    # Split into train and validation\n    split_idx = int(len(sequences) * split_ratio)\n    train_data = sequences[:split_idx]\n    val_data = sequences[split_idx:]\n    \n    print(f\"Total sequences: {len(sequences)}\")\n    print(f\"Training sequences: {len(train_data)}\")\n    print(f\"Validation sequences: {len(val_data)}\")\n    \n    return train_data, val_data\n\n# Test the function\ntrain_data, val_data = load_and_split_data(tokenizer)",
            "block_group": "a0df165620e04083b16decb0bd3637b2",
            "execution_count": 10,
            "outputs": [
                {
                    "name": "stdout",
                    "text": "Total sequences: 3005\nTraining sequences: 2704\nValidation sequences: 301\n",
                    "output_type": "stream"
                }
            ],
            "outputs_reference": "dbtable:cell_outputs/b7959deb-507b-4382-b7f7-f530a17f0a1a",
            "content_dependencies": null
        },
        {
            "cell_type": "code",
            "metadata": {
                "source_hash": "c94e7070",
                "execution_start": 1745505470624,
                "execution_millis": 11204,
                "sql_integration_id": "",
                "execution_context_id": "5c07c2dd-95b5-4d75-8e71-757b6df90be6",
                "deepnote_variable_name": "",
                "cell_id": "c0ba65e3490d41719155b2b1df98f714",
                "deepnote_cell_type": "code"
            },
            "source": "# Now we can try training the model with our data\nfrom pathlib import Path\n\n# Create checkpoints directory if it doesn't exist\nPath(\"checkpoints\").mkdir(exist_ok=True)\n\n# Start training\ntrain_model(\n    model=model,\n    train_data=train_data,\n    val_data=val_data,\n    n_epochs=3,\n    batch_size=32,\n    learning_rate=3e-4,\n    warmup_steps=100,\n    eval_interval=100,\n    save_interval=500,\n    checkpoint_dir='checkpoints'\n)",
            "block_group": "bab44643b662474ca5a3dd7edacdeb8f",
            "execution_count": null,
            "outputs": [
                {
                    "name": "stderr",
                    "text": "Epoch 1/3:   0%|          | 0/85 [00:00<?, ?it/s]",
                    "output_type": "stream"
                }
            ],
            "outputs_reference": "dbtable:cell_outputs/43d7f0ac-63d7-4992-a9f4-3e8033fb441d",
            "content_dependencies": null
        },
        {
            "cell_type": "markdown",
            "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=5a923484-3c6f-40ab-ba4a-906a4dff832d' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
            "metadata": {
                "created_in_deepnote_cell": true,
                "deepnote_cell_type": "markdown"
            }
        }
    ],
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "deepnote_notebook_id": "41ae46fb6ee54803957520ce9bf15416"
    }
}